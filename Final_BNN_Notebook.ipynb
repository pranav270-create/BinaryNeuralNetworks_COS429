{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_BNN_Notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piSBrNP-kvxM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import argparse\n",
        "import matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We submitted these files along with this notebook\n",
        "\n",
        "from Adam import Adam_Metaplastic\n",
        "from data_utils import *\n",
        "from BNN_Conv import *"
      ],
      "metadata": {
        "id": "08mvODdekz5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Task Sequence"
      ],
      "metadata": {
        "id": "_Zhz4DgWMQpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SEQUENCE OF TASKS TO TRAIN ON\n",
        "task_sequence = ['pFMNIST', 'pFMNIST', 'pFMNIST', 'pFMNIST', 'pFMNIST']"
      ],
      "metadata": {
        "id": "c1OnWs8zlALP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load in Training Tasks"
      ],
      "metadata": {
        "id": "D2rhC9SglHaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code borrowed from Laborieux, et al. and slightly modified for our use case.\n",
        "\n",
        "train_loader_list = []\n",
        "test_loader_list = []\n",
        "dset_train_list = []\n",
        "task_names = []\n",
        "\n",
        "for idx, task in enumerate(task_sequence):\n",
        "    if task == 'MNIST':\n",
        "        train_loader_list.append(mnist_train_loader)\n",
        "        test_loader_list.append(mnist_test_loader)\n",
        "        dset_train_list.append(mnist_dset_train)\n",
        "        task_names.append(task)\n",
        "    elif task == 'USPS':\n",
        "        train_loader_list.append(usps_train_loader)\n",
        "        test_loader_list.append(usps_test_loader)\n",
        "        dset_train_list.append(usps_dset_train)\n",
        "        task_names.append(task)\n",
        "    elif task == 'FMNIST':\n",
        "        train_loader_list.append(fashion_mnist_train_loader)\n",
        "        test_loader_list.append(fashion_mnist_test_loader)\n",
        "        dset_train_list.append(fmnist_dset_train)\n",
        "        task_names.append(task)\n",
        "    elif task == 'pMNIST':\n",
        "        train_loader, test_loader, dset_train = create_permuted_loaders(task[1:])\n",
        "        train_loader_list.append(train_loader)\n",
        "        test_loader_list.append(test_loader)\n",
        "        dset_train_list.append(dset_train)\n",
        "        task_names.append(task+str(idx+1))\n",
        "    elif task == 'pFMNIST':\n",
        "        train_loader, test_loader, dset_train = create_permuted_loaders(task[1:])\n",
        "        train_loader_list.append(train_loader)\n",
        "        test_loader_list.append(test_loader)\n",
        "        dset_train_list.append(dset_train)\n",
        "        task_names.append(task+str(idx+1))\n",
        "    elif task == 'pUSPS':\n",
        "        train_loader, test_loader, dset_train = create_permuted_loaders(task[1:])\n",
        "        train_loader_list.append(train_loader)\n",
        "        test_loader_list.append(test_loader)\n",
        "        dset_train_list.append(dset_train)\n",
        "        task_names.append(task+str(idx+1))\n",
        "    elif task == 'animals':\n",
        "        animals_train_loader, animals_test_loader, animals_dset_train = process_cifar10(task)\n",
        "        train_loader_list.append(animals_train_loader)\n",
        "        test_loader_list.append(animals_test_loader)\n",
        "        dset_train_list.append(animals_dset_train)\n",
        "        task_names.append('animals')\n",
        "    elif task == 'vehicles':\n",
        "        vehicles_train_loader, vehicles_test_loader, vehicles_dset_train = process_cifar10(task)\n",
        "        train_loader_list.append(vehicles_train_loader)\n",
        "        test_loader_list.append(vehicles_test_loader)\n",
        "        dset_train_list.append(vehicles_dset_train)\n",
        "        task_names.append('vehicles')\n",
        "    elif 'cifar100' in task:\n",
        "        n_subset = int(task.split('-')[1])  # task = \"cifar100-20\" -> n_subset = 20\n",
        "        train_loader_list, test_loader_list, dset_train_list = process_cifar100(n_subset)\n",
        "        task_names = ['cifar100-'+str(i+1) for i in range(n_subset)]"
      ],
      "metadata": {
        "id": "Op_epecxlCLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ],
      "metadata": {
        "id": "myVVUWsmlPCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "lr = 0.005\n",
        "epochs = 10\n",
        "save_result = True\n",
        "meta = 2\n",
        "archi = [784] + [] + [10]\n",
        "\n",
        "#init = \"normal\"\n",
        "init_width = 0.1\n",
        "decay = 0\n",
        "gamma = 1\n",
        "norm = 'batch'"
      ],
      "metadata": {
        "id": "U_UuvYbplOVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model"
      ],
      "metadata": {
        "id": "Fj5Tt_8HlT53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet(width = init_width).to(device)"
      ],
      "metadata": {
        "id": "7W4o4Ba-lU7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collect initialisation\n",
        "data = {}\n",
        "data['net'] = 'BNN'\n",
        "arch = 'STANDARD'\n",
        "\n",
        "data['arch'] = arch\n",
        "data['norm'] = norm\n",
        "data['lr'], data['meta'], data['task_order'] = [], [], []  \n",
        "data['tsk'], data['epoch'], data['acc_tr'], data['loss_tr'] = [], [], [], []\n",
        "\n",
        "for i in range(len(test_loader_list)):\n",
        "    data['acc_test_tsk_'+str(i+1)], data['loss_test_tsk_'+str(i+1)] = [], []\n",
        "\n",
        "name = '_'+data['net']+'_'+data['arch']+'_'\n",
        "\n",
        "for t in range(len(task_names)):\n",
        "    if ('cifar100' in task_names[t]) and ('cifar100' in name):\n",
        "        pass\n",
        "    else:\n",
        "        name = name+task_names[t]+'-'\n",
        "\n",
        "bn_states = []\n",
        "\n",
        "lrs = [lr*(gamma**(-i)) for i in range(len(train_loader_list))]"
      ],
      "metadata": {
        "id": "GZcsWX2OlXkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test Functions\n"
      ],
      "metadata": {
        "id": "Apb8gxdhlnC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, current_task_index, optimizer, device,\n",
        "          prev_cons=None, prev_params=None, path_integ=None, criterion = torch.nn.CrossEntropyLoss()):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        total_loss = loss        \n",
        "\n",
        "        total_loss.backward()\n",
        "        \n",
        "        # This loop is for BNN parameters having 'org' attribute\n",
        "        for p in list(model.parameters()): # blocking weights with org value greater than a threshold by setting grad to 0 \n",
        "            if hasattr(p,'org'):\n",
        "                p.data.copy_(p.org)\n",
        "                \n",
        "        optimizer.step()\n",
        "\n",
        "        # This loop is only for BNN parameters as they have 'org' attribute\n",
        "        for p in list(model.parameters()):  # updating the org attribute\n",
        "            if hasattr(p,'org'):\n",
        "                p.org.copy_(p.data)"
      ],
      "metadata": {
        "id": "FLJiS8pTlks0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, device, task_idx, criterion = torch.nn.CrossEntropyLoss(reduction='sum'), verbose = False):\n",
        "    \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    for data, target in test_loader:\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item() # mean batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = round( 100. * float(correct) / len(test_loader.dataset)  , 2)\n",
        "    \n",
        "    if len(test_loader.dataset)==60000:\n",
        "      print(f\"Task_idx: {task_idx}\")\n",
        "      print('Train accuracy: {}/{} ({:.2f}%)'.format(\n",
        "          correct, len(test_loader.dataset),\n",
        "          test_acc))\n",
        "    else:\n",
        "      print(f\"Task_idx: {task_idx}\")\n",
        "      print('Test accuracy: {}/{} ({:.2f}%)'.format(\n",
        "          correct, len(test_loader.dataset),\n",
        "          test_acc))\n",
        "      \n",
        "    return test_acc, test_loss"
      ],
      "metadata": {
        "id": "zlB9rTo5luvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Model"
      ],
      "metadata": {
        "id": "zhwWhZPBl0u8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified code from Laborieux, et al.\n",
        "\n",
        "for task_idx, task in enumerate(train_loader_list):\n",
        "    optimizer = Adam_Metaplastic(model.parameters(), lr = lrs[task_idx], meta = meta, weight_decay = decay)\n",
        "           \n",
        "    for epoch in range(1, epochs+1):\n",
        "      train(model, task, task_idx, optimizer, device)\n",
        "\n",
        "      data['task_order'].append(task_idx+1)\n",
        "      data['tsk'].append(task_names[task_idx])\n",
        "      data['epoch'].append(epoch)\n",
        "      data['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "      \n",
        "      print(f\"EPOCH: {epoch}\")\n",
        "      train_accuracy, train_loss = test(model, task, device, task_idx,verbose=True)\n",
        "      \n",
        "      data['acc_tr'].append(train_accuracy)\n",
        "      data['loss_tr'].append(train_loss)\n",
        "      data['meta'].append(meta)\n",
        "      #current_bn_state = model.save_bn_states()\n",
        "  \n",
        "      for other_task_idx, other_task in enumerate(test_loader_list):\n",
        "        test_accuracy, test_loss = test(model, other_task, device, other_task_idx, verbose=(other_task_idx==task_idx))\n",
        "\n",
        "        data['acc_test_tsk_'+str(other_task_idx+1)].append(test_accuracy)\n",
        "        data['loss_test_tsk_'+str(other_task_idx+1)].append(test_loss)\n"
      ],
      "metadata": {
        "id": "6kMSNIcNl1mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output to JSON File"
      ],
      "metadata": {
        "id": "fB6vE9lhl8VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load json module\n",
        "print(data)\n",
        "import json\n",
        "\n",
        "# create json object from dictionary\n",
        "filename = 'pFMNIST_2'\n",
        "file_name = open(str(filename), \"w\")\n",
        "json = json.dump(data, file_name)\n",
        "file_name.close()"
      ],
      "metadata": {
        "id": "CGa-LTJ5mAB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Results"
      ],
      "metadata": {
        "id": "1X2yTbIRmE0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_acc(filename):\n",
        "  f = open(filename)\n",
        "  plot_data = json.load(f)\n",
        "  \n",
        "  figs, ax = plt.subplots(figsize=(15,8))\n",
        "  ax.plot(plot_data['acc_test_tsk_1'], label = 'Task 1: pFMNIST')\n",
        "  ax.plot(plot_data['acc_test_tsk_2'], label = 'Task 2: pFMNIST')\n",
        "  ax.plot(plot_data['acc_test_tsk_3'], label = 'Task 3: pFMNIST')\n",
        "  ax.plot(plot_data['acc_test_tsk_4'], label = 'Task 4: pFMNIST')\n",
        "  ax.plot(plot_data['acc_test_tsk_5'], label = 'Task 5: pFMNIST')\n",
        "  # ax.plot(plot_data['acc_test_tsk_6'], label = 'Task 6: pUSPS')\n",
        "  ax.legend(loc = 'best')\n",
        "  ax.set_ylabel('Test Accuracy', size=15)\n",
        "  ax.set_xlabel('Epoch', size=15)\n",
        "  ax.set_title('Test Accuracy of Sequentially Trained pFMNIST, m=0', size=15)"
      ],
      "metadata": {
        "id": "zye75XFvmGjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Compute Shannon Entropy of Dataset\n",
        "\n",
        "Must first define class labels to pass in, and must import datasets from Keras or external source."
      ],
      "metadata": {
        "id": "PTqpFoTtmPVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in Datasets\n",
        "(mnist_train_x, mnist_train_y), (mnist_test_x, mnist_test_y) = tf.keras.datasets.mnist.load_data()\n",
        "(fmnist_train_x, fmnist_train_y), (fmnist_test_x, fmnist_test_y) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(c10_train_x, c10_train_y), (c10_test_x, c10_test_y) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Define class labels for each dataset\n",
        "mnist_labels = {x: str(x) for x in range(10)}\n",
        "\n",
        "fmnist_labels = {0:\t'T-shirt/top',\n",
        "          1:\t'Trouser',\n",
        "          2:\t'Pullover',\n",
        "          3: 'Dress',\n",
        "          4:\t'Coat',\n",
        "          5:\t'Sandal',\n",
        "          6:\t'Shirt',\n",
        "          7:\t'Sneaker',\n",
        "          8:\t'Bag',\n",
        "          9:\t'Ankle boot'\n",
        "}\n",
        "\n",
        "cifar10_labels = {0:\t'airplane',\n",
        "          1:\t'automobile',\n",
        "          2:\t'bird',\n",
        "          3: 'cat',\n",
        "          4:\t'deer',\n",
        "          5:\t'dog',\n",
        "          6:\t'frog',\n",
        "          7:\t'horse',\n",
        "          8:\t'ship',\n",
        "          9:\t'truck'\n",
        "}"
      ],
      "metadata": {
        "id": "sHCHJblhme75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.measure import shannon_entropy\n",
        "def calculate_class_entropies(x, y, class_labels):\n",
        "  \"\"\" Given images (x) and labels (y), calculate shannon entropies for each class.\n",
        "  Returns class_entropies, a DataFrame of classes and their entropies.\n",
        "  \"\"\"\n",
        "\n",
        "  class_entropies = pd.DataFrame.from_dict(class_labels, orient='index')\n",
        "  class_entropies = class_entropies.rename(columns={0: \"class\"})\n",
        "\n",
        "  entropies = []\n",
        "  for idx, class_labels in class_labels.items():\n",
        "    images = x[np.where(y==idx)[0]]\n",
        "    entropy = np.mean([shannon_entropy(img) for img in images])\n",
        "    entropies.append(entropy)\n",
        "\n",
        "  class_entropies['entropy'] = entropies\n",
        "  return class_entropies"
      ],
      "metadata": {
        "id": "-1bSRuNbmYXU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}